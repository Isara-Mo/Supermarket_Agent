import os
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_core.tools import create_retriever_tool
from langchain.agents import create_agent
from langchain_community.chat_models.tongyi import ChatTongyi
from data_manager import DB_FILE
from langchain_core.messages import HumanMessage
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse

import numpy as np
import onnxruntime as ort
from transformers import BertTokenizer
import contextvars

# å®šä¹‰ä¸€ä¸ªä¸Šä¸‹æ–‡å˜é‡ï¼Œé»˜è®¤å€¼ä¸º 0
complexity_context = contextvars.ContextVar("complexity", default=0)
# ======================
# æ¨¡å—çº§â€œç§æœ‰â€å˜é‡
# ======================
_tokenizer = None
_session = None
_qwen_fast_model = ChatTongyi(model="qwen-flash",api_key="")                    #api_keyè°ƒç”¨æš‚æ—¶è¿˜ä¸å¤ªè§„èŒƒ
_qwen_max_model = ChatTongyi(model="qwen3-max",api_key='')
def init_embeddings(dashscope_api_key):
    return DashScopeEmbeddings(model="text-embedding-v1", dashscope_api_key=dashscope_api_key)

##æ–‡æœ¬åˆ†ç±»æ¨¡å—
def init_model(
    model_name="bert-base-chinese",
    onnx_path="bert_classifier.onnx",
    providers=None
):
    """
    åœ¨ç¨‹åºå¯åŠ¨æ—¶è°ƒç”¨ä¸€æ¬¡
    """
    global _tokenizer, _session

    if providers is None:
        providers = ["CPUExecutionProvider"]

    _tokenizer = BertTokenizer.from_pretrained(model_name)
    _session = ort.InferenceSession(onnx_path, providers=providers)

def softmax(x):
    e = np.exp(x - np.max(x))
    return e / e.sum()

def predict(text):
    if _tokenizer is None or _session is None:
        raise RuntimeError("Model not initialized, call init_model() first")
    inputs = _tokenizer(
        text,
        return_tensors="np",      # ç”Ÿæˆ numpy
        padding="max_length",
        truncation=True,
        max_length=256
    )

    # ğŸ”´ å…³é”®ï¼šæ˜¾å¼è½¬æˆ int64
    input_ids = inputs["input_ids"].astype("int64")
    attention_mask = inputs["attention_mask"].astype("int64")

    ort_inputs = {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
    }

    logits = _session.run(None, ort_inputs)[0]
    probs = softmax(logits[0])
    pred = int(np.argmax(probs))

    return pred, probs

##
@wrap_model_call
def dynamic_deepseek_routing(request: ModelRequest, handler) -> ModelResponse:
    """
    æ ¹æ®å¯¹è¯å¤æ‚åº¦åŠ¨æ€é€‰æ‹© DeepSeek æ¨¡å‹ï¼š
    - å¤æ‚ï¼šdeepseek-reasoner
    - ç®€å•ï¼šdeepseek-chat
    """
    # æ ¹æ®é¢„æµ‹ç»“æœé€‰æ‹©æ¨¡å‹
    current_pred = complexity_context.get()
    if current_pred == 1:  # å¦‚æœæ˜¯å¤æ‚é—®é¢˜
        request.model = _qwen_max_model
    else:  # å¦‚æœæ˜¯ç®€å•é—®é¢˜
        request.model = _qwen_fast_model
    
    print(f"æ­¤æ¬¡çš„æ¨¡å‹: {request.model}")
    # è°ƒç”¨è¢«åŒ…è£¹çš„ä¸‹æ¸¸ï¼ˆçœŸæ­£çš„æ¨¡å‹è°ƒç”¨ï¼‰
    return handler(request)
def _get_last_user_text(messages) -> str:
        """ä»æ¶ˆæ¯åˆ—è¡¨ä¸­å–æœ€è¿‘ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯æ–‡æœ¬ï¼ˆæ— åˆ™è¿”å›ç©ºä¸²ï¼‰"""
        for m in reversed(messages):
            if isinstance(m, HumanMessage):
                # content å¯èƒ½æ˜¯çº¯å­—ç¬¦ä¸²æˆ–å¯Œå†…å®¹ï¼›è¿™é‡Œåªå¤„ç†ä¸ºå­—ç¬¦ä¸²çš„å¸¸è§æƒ…å†µ
                return m.content if isinstance(m.content, str) else ""
        return ""
def _get_last_text(messages) -> str:
    """å¦‚æœ messages çš„æœ€åä¸€æ¡æ˜¯ HumanMessageï¼Œåˆ™è¿”å›1ï¼Œå¦åˆ™0"""
    if not messages:
        return 0

    last = messages[-1]
    if isinstance(last, HumanMessage):
        return 1 if isinstance(last.content, str) else 0

    return 0

def create_supermarket_agent(dashscope_api_key, db_name):
    """ã€å·¥å‚å‡½æ•°ã€‘è´Ÿè´£ä»é›¶åˆ›å»ºä¸€ä¸ª Agent å¯¹è±¡"""
    # 1. åˆå§‹åŒ–æ¨¡å‹
    llm = ChatTongyi(model="qwen-plus")
    embeddings = init_embeddings(dashscope_api_key)
    
    # 2. åŠ è½½å‘é‡æ•°æ®åº“
    db_path = os.path.join(DB_FILE, db_name)
    db = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
    
    # 3. åˆ›å»ºå·¥å…·
    tool = create_retriever_tool(
        db.as_retriever(search_kwargs={"k": 5}), 
        "product_search", 
        "æœç´¢å•†å“ä¿¡æ¯ï¼ŒåŒ…æ‹¬åç§°ã€ä»·æ ¼ã€åº“å­˜ã€æè¿°ç­‰"
    )
    
    # 4. æ„å»º Agent
    agent = create_agent(
        model=llm, 
        tools=[tool], 
        system_prompt="ä½ æ˜¯ä¸€ä¸ªè¶…å¸‚å®¢æœåŠ©æ‰‹ã€‚æ ¹æ®å•†å“æ•°æ®ç»™å‡ºå‡†ç¡®å‹å¥½çš„å›ç­”ã€‚å¦‚æœæ‰¾ä¸åˆ°å•†å“ï¼Œè¯·ç¤¼è²Œåœ°è¯´æ˜ã€‚",
        middleware=[dynamic_deepseek_routing]
    )
    return agent

def run_supermarket_agent(agent, user_question):
    """ã€æ‰§è¡Œå‡½æ•°ã€‘ç›´æ¥åˆ©ç”¨ä¼ å…¥çš„ agent å¯¹è±¡è¿›è¡Œæ¨ç†"""
    try:
        # ä½¿ç”¨ BERT æ¨¡å‹é¢„æµ‹å¤æ‚åº¦
        global pred, probs
        pred, probs = predict(user_question)
        complexity_context.set(pred)
        print(f"[BERTæ¨ç†] é¢„æµ‹æ ‡ç­¾: {pred}, é¢„æµ‹æ¦‚ç‡: {probs}")

        response = agent.invoke({"messages": [("user", user_question)]})
        return response["messages"][-1].content
    except Exception as e:
        return f"âŒ å¯¹è¯æ‰§è¡Œå‡ºé”™: {str(e)}"